{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2299aa9a-86f3-4b2a-9f27-1e957dad2f90",
   "metadata": {
    "id": "2299aa9a-86f3-4b2a-9f27-1e957dad2f90"
   },
   "source": [
    "## 14. Wind Power Prediction Using the Tiny Time Mixers model\n",
    "\n",
    "**Task Description**:\n",
    "\n",
    "Use the TTM model to forecast wind power generation, incorporating wind speed and power data. At each time point, forecast the hourly output for the next day. Analyze the forecast errors for the different forecast horizons using the RMSE and MAE. Use the last available year of data as your test set.\n",
    "Predict the sum of Offshore_Wind and Onshore_Wind (you can choose whether to forecast both and then sum, or forecast the sum directly) from the file: Realised_Supply_Germany.csv\n",
    "\n",
    "Bonus: Test different feature selection approaches for the weather input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7270e-03d2-451a-9b21-d67230e08863",
   "metadata": {
    "id": "9da7270e-03d2-451a-9b21-d67230e08863"
   },
   "outputs": [],
   "source": [
    "# ensure that Jupyter automatically updates imports when the imported files change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "x6P8YXIhKw8q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31800,
     "status": "ok",
     "timestamp": 1750147656060,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "x6P8YXIhKw8q",
    "outputId": "037932e7-6bf1-49d1-8c59-b8f8323a838a"
   },
   "outputs": [],
   "source": [
    "# # add google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eLL-uClRLZv3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 148548,
     "status": "ok",
     "timestamp": 1750147804604,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "eLL-uClRLZv3",
    "outputId": "f951107f-fce2-48cc-fbbf-cf651f685b1d"
   },
   "outputs": [],
   "source": [
    "# !pip install -q \"granite-tsfm[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mooFHn_lMSCH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1750147906276,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "mooFHn_lMSCH",
    "outputId": "8acdb803-9aab-4d9a-b749-3d23fc811e7c"
   },
   "outputs": [],
   "source": [
    "# # sometimes needed to be run twice in colab\n",
    "# %cd /content/drive/MyDrive/ttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0520a698-48cd-4224-a568-d3b532e53d86",
   "metadata": {
    "id": "0520a698-48cd-4224-a568-d3b532e53d86"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3bd5be-fef1-4409-b6e6-267aea974bf0",
   "metadata": {
    "id": "bc3bd5be-fef1-4409-b6e6-267aea974bf0"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# TTM Model path. The default model path is Granite-R2. Below, you can choose other TTM releases.\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "# TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r1\"\n",
    "# TTM_MODEL_PATH = \"ibm-research/ttm-research-r2\"\n",
    "\n",
    "# Context length, Or Length of the history.\n",
    "# Currently supported values are: 512/1024/1536 for Granite-TTM-R2 and Research-Use-TTM-R2, and 512/1024 for Granite-TTM-R1\n",
    "CONTEXT_LENGTH = 512\n",
    "\n",
    "# Granite-TTM-R2 supports forecast length upto 720 and Granite-TTM-R1 supports forecast length upto 96\n",
    "PREDICTION_LENGTH = 24 # \"hourly output for the next day\"\n",
    "\n",
    "dataset_path = \"data/Realised_Supply_Germany.csv\"\n",
    "weather_path = \"data/Weather_Data_Germany.csv\"\n",
    "weather_path_2 = \"data/Weather_Data_Germany_2022.csv\"\n",
    "# Results dir\n",
    "OUT_DIR = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8d16323-76b2-4156-9837-a9068065908d",
   "metadata": {
    "id": "f8d16323-76b2-4156-9837-a9068065908d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path, on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe3546c-6185-49ef-b131-8f0d51beaeb6",
   "metadata": {
    "id": "cbe3546c-6185-49ef-b131-8f0d51beaeb6"
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv(weather_path)\n",
    "weather2022 = pd.read_csv(weather_path_2)\n",
    "weather = pd.concat([weather, weather2022], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e2e45d-d4ef-4b75-872e-ae92c8d159b7",
   "metadata": {
    "id": "54e2e45d-d4ef-4b75-872e-ae92c8d159b7"
   },
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Looking at wind power generation data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a44223-0d09-4a6a-a79d-142f867ecf2c",
   "metadata": {
    "id": "90a44223-0d09-4a6a-a79d-142f867ecf2c"
   },
   "outputs": [],
   "source": [
    "from plot_utils import wind_data_summary, create_wind_eda_plots, explain_data_completeness, wind_scatter_analysis\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "df_processed = wind_data_summary(df)\n",
    "create_wind_eda_plots(df_processed)\n",
    "completeness = explain_data_completeness(df_processed)\n",
    "wind_correlation = wind_scatter_analysis(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d18adb-f7b2-496b-9e0f-92a065d67eee",
   "metadata": {
    "id": "b9d18adb-f7b2-496b-9e0f-92a065d67eee"
   },
   "source": [
    "Wind Offshore: Relatively consistent (Coefficient of Variation = 0.23), stable baseline ~414 MW, no missing values\n",
    "Wind Onshore: Extremely variable (Coefficient of Variation = 1.96), low baseline ~99 MW, highly irregular\n",
    "\n",
    "Consider forecasting them separately then summing, rather than forecasting the total directly. Offshore wind is the dominant, predictable component while onshore is the volatile component.\n",
    "\n",
    "robust scaling and consider log transformation for onshore wind to handle the extreme right skew\n",
    "\n",
    "Potentially doing something about DST irregularities? (1) fill gaps, 2) convert to UTC?\n",
    "\n",
    "Seasonalities: Yearly (summer slightly higher than winter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54823b31-b519-4701-9759-e18540dc5b0b",
   "metadata": {
    "id": "54823b31-b519-4701-9759-e18540dc5b0b"
   },
   "source": [
    "### üå§Ô∏è Weather Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b1d35-4d96-4151-8ee5-9b83c25ee93d",
   "metadata": {
    "id": "002b1d35-4d96-4151-8ee5-9b83c25ee93d"
   },
   "source": [
    "\n",
    "| **Column**         | **Description**                                                                 | **Usage for TTM**                                           |\n",
    "|--------------------|----------------------------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| `longitude`, `latitude` | Geographical coordinates of the forecast grid point                        | Use for **spatial alignment** or **interpolation** to plant |\n",
    "| `forecast_origin`   | Time when the forecast was issued                                               | Use to determine **forecast lead time**                     |\n",
    "| `time`              | Valid time of the forecast (i.e., when the forecast applies)                    | Align with **target wind power timestamp**                  |\n",
    "| `cdir`              | Cardinal wind direction (¬∞)                                                     | Optional directional feature                                |\n",
    "| `z`                 | Geopotential height                                                             | Rarely needed for power; may relate to elevation effects    |\n",
    "| `msl`               | Mean sea level pressure (hPa)                                                   | Could help model pressure-driven wind patterns              |\n",
    "| `blh`               | Boundary layer height                                                           | Indicates turbulence layer; optional                        |\n",
    "| `tcc`               | Total cloud cover (0‚Äì1)                                                         | Useful proxy for sunlight ‚Üí solar gain interference         |\n",
    "| `u10`, `v10`        | 10m wind speed, u-component (east-west), v-component (north-south) (m/s)                                                       | **Primary feature** for wind power                          |\n",
    "| `t2m`               | Temperature at 2 meters (K)                                                     | Optional; affects air density                               |\n",
    "| `ssr`, `tsr`, `sund`| Surface/Top solar radiation & sunshine duration                                 | Optional; mostly for solar models                           |\n",
    "| `tp`                | Total precipitation                                                             | Might relate to wind instability                            |\n",
    "| `fsr`               | Forecast solar radiation                                                        | See above                                                   |\n",
    "| `u100`, `v100`      | 100m wind components (m/s), u-component (east-west), v-component (north-south)                                                      | **Highly relevant** ‚Äî turbines operate around this height   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8ccec-b068-4114-b429-651081ed65b8",
   "metadata": {
    "id": "28d8ccec-b068-4114-b429-651081ed65b8"
   },
   "outputs": [],
   "source": [
    "from plot_utils import plot_weather_on_real_map, create_interactive_wind_analysis\n",
    "coord_analysis = plot_weather_on_real_map(weather)\n",
    "interactive_map = create_interactive_wind_analysis(coord_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6bc512-9487-40bd-883c-5dde42d92c92",
   "metadata": {
    "id": "5f6bc512-9487-40bd-883c-5dde42d92c92"
   },
   "source": [
    "-> Clear north-south wind resource gradient  \n",
    "-> Lower pressure = higher winds (weather systems)  \n",
    "-> 10m + 100m wind strongly correlate, might be redundant to use both  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c128ac-2880-41b9-b788-82558b834a54",
   "metadata": {
    "id": "40c128ac-2880-41b9-b788-82558b834a54"
   },
   "source": [
    "### Data Processing - ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7qJ53x1spfKI",
   "metadata": {
    "id": "7qJ53x1spfKI"
   },
   "outputs": [],
   "source": [
    "from tsfm_public import TimeSeriesPreprocessor, TrackingCallback, count_parameters, get_datasets\n",
    "from tsfm_public.toolkit.get_model import get_model\n",
    "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
    "from tsfm_public.toolkit.visualization import plot_predictions\n",
    "\n",
    "from preprocess import clean_wind_data, create_weather_features_simple, merge_wind_weather_data, create_temporal_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600194ee-667d-4e84-8bf8-130a2f3c1f1a",
   "metadata": {
    "id": "600194ee-667d-4e84-8bf8-130a2f3c1f1a"
   },
   "outputs": [],
   "source": [
    "# TODO: create additional features (time of day, forecast lead time, lags) - look at medium article linked by her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d84f794d-a17e-4ced-bff7-a05abb408854",
   "metadata": {
    "id": "d84f794d-a17e-4ced-bff7-a05abb408854"
   },
   "outputs": [],
   "source": [
    "from preprocess import clean_wind_data, create_weather_features_simple, merge_wind_weather_data, create_temporal_splits\n",
    "def setup_ttm_preprocessor(df_combined, context_length=512, prediction_length=24, timestamp_column=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Set up TimeSeriesPreprocessor for TTM model\n",
    "    \"\"\"\n",
    "    print(\"=== Setting up TTM preprocessor ===\")\n",
    "\n",
    "    context_length = 512 # 512 hours ‚âà 21 days\n",
    "    prediction_length = 24  # Next 24 hours\n",
    "\n",
    "    # Column specifications\n",
    "    id_columns = []  # Single aggregated time series\n",
    "\n",
    "    # Target: What we want to forecast\n",
    "    target_columns = [\"wind_power_offshore\", \"wind_power_onshore\"]\n",
    "\n",
    "    # Observable: Weather variables (known in future via forecasts)\n",
    "    # Since we have weather forecasts for all time periods, all weather features are observable\n",
    "    observable_columns = [\n",
    "        \"wind_speed_100m\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"pressure_hpa\",\n",
    "        \"temperature_c\",\n",
    "        \"wind_dir_sin\",  # Wind direction is observable (from forecasts)\n",
    "        \"wind_dir_cos\",  # Wind direction is observable (from forecasts)\n",
    "        \"u100\", \"v100\",\n",
    "        \"u10\", \"v10\",\n",
    "        \"blh\",\n",
    "        \"tcc\",\n",
    "        \"tp\",  # precipitation is also observable from forecasts\n",
    "        \"forecast_lead_hours\"  # forecast lead time as feature\n",
    "    ]\n",
    "\n",
    "    # Conditional: Variables known in past but not future\n",
    "    # Since we have weather forecasts, most variables are observable rather than conditional\n",
    "    conditional_columns = []\n",
    "\n",
    "    # Control: None for this use case\n",
    "    control_columns = []\n",
    "\n",
    "    column_specifiers = {\n",
    "        \"timestamp_column\": timestamp_column,\n",
    "        \"id_columns\": id_columns,\n",
    "        \"target_columns\": target_columns,\n",
    "        \"observable_columns\": observable_columns,\n",
    "        \"conditional_columns\": conditional_columns,\n",
    "        \"control_columns\": control_columns,\n",
    "    }\n",
    "\n",
    "    print(\"TTM Column Configuration:\")\n",
    "    print(f\"Target columns ({len(target_columns)}): {target_columns}\")\n",
    "    print(f\"Observable columns ({len(observable_columns)}): {observable_columns}\")\n",
    "    print(f\"Conditional columns ({len(conditional_columns)}): {conditional_columns}\")\n",
    "    print(f\"Context length: {context_length} hours (~{context_length/24:.1f} days)\")\n",
    "    print(f\"Prediction length: {prediction_length} hours\")\n",
    "\n",
    "    # Create preprocessor\n",
    "    tsp = TimeSeriesPreprocessor(\n",
    "        **column_specifiers,\n",
    "        context_length=context_length,\n",
    "        prediction_length=prediction_length,\n",
    "        scaling=True,  # Important for different units\n",
    "        encode_categorical=False,  # No categorical data\n",
    "        scaler_type=\"standard\",  # Standardize all features\n",
    "    )\n",
    "\n",
    "    return tsp, column_specifiers, context_length, prediction_length\n",
    "\n",
    "# Main processing pipeline\n",
    "def prepare_wind_power_dataset(df_wind, df_weather):\n",
    "    \"\"\"\n",
    "    Complete pipeline to prepare wind power dataset for TTM\n",
    "    \"\"\"\n",
    "    print(\"=== WIND POWER TTM DATA PREPARATION PIPELINE ===\\n\")\n",
    "\n",
    "    print(\"Step 1: Cleaning and resampling wind power data (15min -> hourly)\")\n",
    "    df_wind_clean = clean_wind_data(df_wind)\n",
    "\n",
    "    print(\"\\nStep 2: Creating spatially averaged weather features\")\n",
    "    df_weather_agg = create_weather_features_simple(df_weather)\n",
    "\n",
    "    print(\"\\nStep 3: Merging datasets\")\n",
    "    df_combined = merge_wind_weather_data(df_wind_clean, df_weather_agg)\n",
    "\n",
    "    print(\"\\nStep 4: Creating temporal splits\")\n",
    "    df_combined, split_config = create_temporal_splits(df_combined, plot=False)\n",
    "    # The following is model specific:\n",
    "    print(\"\\nStep 5: Setting up TTM preprocessor\")\n",
    "    tsp, column_specifiers, context_length, prediction_length = setup_ttm_preprocessor(df_combined)\n",
    "\n",
    "    # Final check: ensure data is properly sorted\n",
    "    if not df_combined['timestamp'].is_monotonic_increasing:\n",
    "        print(\"\\nFinal sorting check: Re-sorting data by timestamp\")\n",
    "        df_combined = df_combined.sort_values('timestamp').reset_index(drop=True)\n",
    "    else:\n",
    "        print(\"\\nFinal sorting check: Data already properly sorted\")\n",
    "\n",
    "    print(f\"\\n‚úÖ PIPELINE COMPLETE\")\n",
    "    print(f\"Final dataset: {df_combined.shape}\")\n",
    "    print(f\"Temporal resolution: Hourly\")\n",
    "    print(f\"Spatial coverage: Germany (averaged)\")\n",
    "\n",
    "    return {\n",
    "        'data': df_combined,\n",
    "        'preprocessor': tsp,\n",
    "        'split_config': split_config,\n",
    "        'column_specifiers': column_specifiers,\n",
    "        'context_length': context_length,\n",
    "        'prediction_length': prediction_length\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c61ec-a027-4a85-81af-d002070d0a08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13365,
     "status": "ok",
     "timestamp": 1750148022139,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "e71c61ec-a027-4a85-81af-d002070d0a08",
    "outputId": "a06f86ca-e549-4a66-8c28-bddd3d50c449"
   },
   "outputs": [],
   "source": [
    "df_wind = df.copy()\n",
    "df_weather = weather.copy()\n",
    "result = prepare_wind_power_dataset(df_wind, df_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac424838-751b-4309-9d92-8c38eb63b09b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "510b0d0e9c5a4df1888f6379b875ad49",
      "5a89c34371674fc0b1ce5068ebf526a4",
      "f82a90d48fb94dd3bec2bd17c31ed2bc",
      "e9ecdf4cb3ca4f29ae91248da74d62f2",
      "2cdf6461a1c64f0d9a2d6dc55e62b5a0",
      "ca49952ab2db4e3da15db67c03146934",
      "c2d6cdfdb6e242a5a4fbcc92c445d509",
      "2188739422404aa9865e58cc66f7fc8e",
      "67216125c2d644b091cdbcb2133f0112",
      "2e6498723d964ead82ef0c1971265831",
      "8135ff20b9374d7a99070fad1691cb35",
      "3d6f6c3b7af2495c85cb5758b3ef0e58",
      "f78e35d4d1254c8fb2172214bfdb4b1f",
      "23824dcdd2d4432c86efd8c6d6eaaa37",
      "6a4db443cce94842b00d3ab6de3193c5",
      "2e342bc24924404c954b57eceed2c08e",
      "750fdcda161445feb5cdf259bf5c230d",
      "0f3c3a65081940fca2853f6cebbe84b8",
      "5cb90eb7730a41afac3f51b906f585be",
      "3bb25e03a9c14d9d8a145ac6c84ee601",
      "a0ff0b650c764d3f918545504274baca",
      "cf5b43e033214235ac4e54d293b2f68b"
     ]
    },
    "executionInfo": {
     "elapsed": 6844,
     "status": "ok",
     "timestamp": 1750148028967,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "ac424838-751b-4309-9d92-8c38eb63b09b",
    "outputId": "1b7e0879-21ab-47fa-96ae-c9d4ddd58751"
   },
   "outputs": [],
   "source": [
    "print(result.keys())\n",
    "df_combined = result['data']\n",
    "tsp = result['preprocessor']\n",
    "split_config = result['split_config']\n",
    "\n",
    "TARGET_DATASET = \"german_wind_power\"\n",
    "dataset_name=TARGET_DATASET\n",
    "context_length=CONTEXT_LENGTH\n",
    "forecast_length=PREDICTION_LENGTH\n",
    "batch_size=64\n",
    "\n",
    "zeroshot_model = get_model(\n",
    "    TTM_MODEL_PATH,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_length,\n",
    "    freq_prefix_tuning=False,\n",
    "    freq=None,\n",
    "    prefer_l1_loss=False,\n",
    "    prefer_longer_context=True,\n",
    ")\n",
    "\n",
    "# Creates the preprocessed pytorch datasets needed for training and evaluation using the HuggingFace trainer\n",
    "dset_train, dset_valid, dset_test = get_datasets(\n",
    "    tsp, df_combined, split_config, use_frequency_token=zeroshot_model.config.resolution_prefix_tuning\n",
    ")\n",
    "print(type(dset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f183f-3725-46be-80f4-5b4380e91bf8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1750148028967,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "715f183f-3725-46be-80f4-5b4380e91bf8",
    "outputId": "32bfaf0b-37ec-4e74-8c52-0fd74e14e14a"
   },
   "outputs": [],
   "source": [
    "# print(f\"\\nPreprocessor target columns: {tsp.target_columns}\")\n",
    "# print(f\"Preprocessor observable columns: {getattr(tsp, 'observable_columns', 'Not set')}\")\n",
    "# print(f\"All feature columns: {getattr(tsp, 'feature_columns', 'Not set')}\")\n",
    "# print(\"\\n2Ô∏è‚É£ TEST DATASET STRUCTURE:\")\n",
    "# print(f\"Dataset type: {type(dset_test)}\")\n",
    "# print(f\"Dataset length: {len(dset_test)}\")\n",
    "# sample = dset_test[0]\n",
    "# print(f\"\\nSample structure: {type(sample)}\")\n",
    "# if isinstance(sample, dict):\n",
    "#     for key, value in sample.items():\n",
    "#         if hasattr(value, 'shape'):\n",
    "#             print(f\"  {key}: {value.shape} (dtype: {value.dtype})\")\n",
    "#         else:\n",
    "#             print(f\"  {key}: {type(value)}\")\n",
    "# # vars(zeroshot_trainer)\n",
    "# dset_test[0]['past_values'].shape, dset_test[0]['future_values'].shape, len(dset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31031f-ebbb-405a-a6c2-9bac4ca3f519",
   "metadata": {
    "id": "6e31031f-ebbb-405a-a6c2-9bac4ca3f519"
   },
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     print(f\"labels length: {len(labels)}\")\n",
    "#     for item in predictions:\n",
    "#         print(type(item))\n",
    "#         print(item.shape)\n",
    "#     print(f\"len(predictions) {len(predictions)}\")\n",
    "#     print(f\"type(predictions) {type(predictions)}\")\n",
    "#     # print(f\"predictions {predictions}\")\n",
    "#     print(f\"len(labels) {len(labels)}\")\n",
    "#     mse = ((predictions - labels) ** 2).mean()\n",
    "#     return {\"custom_mse\": mse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522143e1-40b6-4a8e-8861-08f5d4bf206f",
   "metadata": {
    "id": "522143e1-40b6-4a8e-8861-08f5d4bf206f"
   },
   "outputs": [],
   "source": [
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "zeroshot_trainer = Trainer(\n",
    "    model=zeroshot_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=temp_dir,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8082f-a6f6-4128-86a2-40ec70a0b32b",
   "metadata": {
    "id": "06c8082f-a6f6-4128-86a2-40ec70a0b32b"
   },
   "outputs": [],
   "source": [
    "# # TODO: t+h step ahead metrics RMSE + MAE - maybe change loss metric from mse to mae?\n",
    "# # evaluate = zero-shot performance\n",
    "# print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "# zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "# print(zeroshot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e3cc8-e501-41fa-a2d7-4db6181fbf5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 11402,
     "status": "ok",
     "timestamp": 1750148040363,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "271e3cc8-e501-41fa-a2d7-4db6181fbf5c",
    "outputId": "88926132-85cb-41b7-9832-01f20a02077a"
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions_dict = zeroshot_trainer.predict(dset_test)\n",
    "predictions_np = predictions_dict.predictions[0]\n",
    "print(predictions_np.shape)\n",
    "print(f\"    Dimension 0 ({predictions_np.shape[0]}): Number of sliding windows\")\n",
    "print(f\"    Dimension 1 ({predictions_np.shape[1]}): Prediction horizon (hours ahead)\")\n",
    "print(f\"    Dimension 2 ({predictions_np.shape[2]}): Number of channels/features being predicted\")\n",
    "df_test = df_combined.iloc[split_config['test'][0]:split_config['test'][1]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RTM1h49chB-O",
   "metadata": {
    "id": "RTM1h49chB-O"
   },
   "outputs": [],
   "source": [
    "def denormalize_ttm_predictions_simple(predictions_np, tsp):\n",
    "    \"\"\"\n",
    "    Simple denormalization for TTM predictions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_np : numpy array, shape (n_windows, horizon, n_channels)\n",
    "        Raw model predictions in normalized space\n",
    "    tsp : TimeSeriesPreprocessor\n",
    "        Fitted preprocessor with target_scaler_dict\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Denormalized predictions for each target\n",
    "    \"\"\"\n",
    "\n",
    "    # Get target scaler\n",
    "    target_scaler = tsp.target_scaler_dict['0']\n",
    "\n",
    "    # Extract target predictions (first 2 channels)\n",
    "    n_targets = len(tsp.target_columns)\n",
    "    target_preds = predictions_np[:, :, :n_targets]  # (n_windows, 24, 2)\n",
    "\n",
    "    # Reshape and denormalize\n",
    "    n_windows, horizon, _ = target_preds.shape\n",
    "    reshaped = target_preds.reshape(-1, n_targets)\n",
    "    denormalized = target_scaler.inverse_transform(reshaped)\n",
    "    final_shape = denormalized.reshape(n_windows, horizon, n_targets)\n",
    "\n",
    "    # Return as dictionary\n",
    "    result = {}\n",
    "    for i, col in enumerate(tsp.target_columns):\n",
    "        result[col] = final_shape[:, :, i]\n",
    "\n",
    "    return result\n",
    "\n",
    "def plot_clean_comparison(denorm_preds, df_combined, start_idx=0, length=500):\n",
    "    \"\"\"\n",
    "    Clean plot with multiple forecast horizons - 5 prediction lines per target\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    denorm_preds : dict\n",
    "        Denormalized predictions from denormalize_ttm_predictions_simple()\n",
    "    df_combined : pandas DataFrame\n",
    "        Ground truth data\n",
    "    start_idx : int\n",
    "        Starting time step\n",
    "    length : int\n",
    "        Number of time steps to plot\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üìä CREATING MULTI-HORIZON COMPARISON PLOT\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Extract predictions and ground truth\n",
    "    offshore_pred = denorm_preds['wind_power_offshore']  # (n_windows, 24)\n",
    "    onshore_pred = denorm_preds['wind_power_onshore']    # (n_windows, 24)\n",
    "\n",
    "    offshore_truth = df_combined['wind_power_offshore'].values\n",
    "    onshore_truth = df_combined['wind_power_onshore'].values\n",
    "\n",
    "    # Define forecast horizons (1h, 6h, 12h, 18h, 24h ahead)\n",
    "    horizons = [0, 5, 11, 17, 23]  # 0-indexed: 1h, 6h, 12h, 18h, 24h ahead\n",
    "    horizon_labels = ['1h', '6h', '12h', '18h', '24h']\n",
    "    colors = ['red', 'orange', 'green', 'blue', 'purple']\n",
    "\n",
    "    # Create continuous prediction lines for each horizon\n",
    "    continuous_preds = {}\n",
    "\n",
    "    for i, (h_idx, h_label) in enumerate(zip(horizons, horizon_labels)):\n",
    "        continuous_preds[f'offshore_{h_label}'] = offshore_pred[:, h_idx]\n",
    "        continuous_preds[f'onshore_{h_label}'] = onshore_pred[:, h_idx]\n",
    "\n",
    "    print(f\"Ground truth length: {len(offshore_truth)}\")\n",
    "    print(f\"Prediction windows: {offshore_pred.shape[0]}\")\n",
    "    print(f\"Forecast horizons: {horizon_labels}\")\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Plot 1: Offshore Wind Power\n",
    "    plt.subplot(2, 1, 1)\n",
    "\n",
    "    # Ground truth\n",
    "    plot_end = min(start_idx + length, len(offshore_truth))\n",
    "    time_range = range(start_idx, plot_end)\n",
    "\n",
    "    plt.plot(time_range, offshore_truth[start_idx:plot_end],\n",
    "             'k-', linewidth=3, label='Ground Truth Offshore', alpha=0.9, zorder=10)\n",
    "\n",
    "    # Prediction lines for different horizons\n",
    "    for i, (h_idx, h_label, color) in enumerate(zip(horizons, horizon_labels, colors)):\n",
    "        # For h-hour ahead predictions, we need to align with ground truth\n",
    "        # Window i predicts for hour i+h+1, so we start plotting from hour h\n",
    "        horizon_hours = h_idx + 1  # Convert 0-indexed to 1-indexed hours\n",
    "\n",
    "        pred_start = start_idx + horizon_hours\n",
    "        pred_end = min(pred_start + len(continuous_preds[f'offshore_{h_label}']), plot_end)\n",
    "\n",
    "        if pred_start < plot_end and pred_start < len(continuous_preds[f'offshore_{h_label}']):\n",
    "            pred_range = range(pred_start, pred_end)\n",
    "            pred_values = continuous_preds[f'offshore_{h_label}'][start_idx:start_idx + len(pred_range)]\n",
    "\n",
    "            plt.plot(pred_range, pred_values,\n",
    "                    color=color, linewidth=2, linestyle='--', alpha=0.8,\n",
    "                    label=f'Prediction {h_label} ahead', zorder=5-i)\n",
    "\n",
    "    plt.title('Offshore Wind Power: Multi-Horizon Predictions vs Ground Truth', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Time Step (Hours)')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Onshore Wind Power\n",
    "    plt.subplot(2, 1, 2)\n",
    "\n",
    "    # Ground truth\n",
    "    plt.plot(time_range, onshore_truth[start_idx:plot_end],\n",
    "             'k-', linewidth=3, label='Ground Truth Onshore', alpha=0.9, zorder=10)\n",
    "\n",
    "    # Prediction lines for different horizons\n",
    "    for i, (h_idx, h_label, color) in enumerate(zip(horizons, horizon_labels, colors)):\n",
    "        horizon_hours = h_idx + 1\n",
    "\n",
    "        pred_start = start_idx + horizon_hours\n",
    "        pred_end = min(pred_start + len(continuous_preds[f'onshore_{h_label}']), plot_end)\n",
    "\n",
    "        if pred_start < plot_end and pred_start < len(continuous_preds[f'onshore_{h_label}']):\n",
    "            pred_range = range(pred_start, pred_end)\n",
    "            pred_values = continuous_preds[f'onshore_{h_label}'][start_idx:start_idx + len(pred_range)]\n",
    "\n",
    "            plt.plot(pred_range, pred_values,\n",
    "                    color=color, linewidth=2, linestyle='--', alpha=0.8,\n",
    "                    label=f'Prediction {h_label} ahead', zorder=5-i)\n",
    "\n",
    "    plt.title('Onshore Wind Power: Multi-Horizon Predictions vs Ground Truth', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Time Step (Hours)')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate metrics for each horizon\n",
    "    print(f\"\\nüìà MULTI-HORIZON PREDICTION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    metrics_summary = {}\n",
    "\n",
    "    for target in ['offshore', 'onshore']:\n",
    "        print(f\"\\nüéØ {target.upper()} WIND POWER:\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if target == 'offshore':\n",
    "            truth = offshore_truth\n",
    "        else:\n",
    "            truth = onshore_truth\n",
    "\n",
    "        target_metrics = {}\n",
    "\n",
    "        for h_idx, h_label in zip(horizons, horizon_labels):\n",
    "            pred_key = f'{target}_{h_label}'\n",
    "            pred_values = continuous_preds[pred_key]\n",
    "\n",
    "            # Calculate metrics with proper alignment\n",
    "            horizon_hours = h_idx + 1\n",
    "\n",
    "            # Align predictions with ground truth\n",
    "            if len(truth) > horizon_hours and len(pred_values) > 0:\n",
    "                max_len = min(len(truth) - horizon_hours, len(pred_values))\n",
    "\n",
    "                aligned_truth = truth[horizon_hours:horizon_hours + max_len]\n",
    "                aligned_pred = pred_values[:max_len]\n",
    "\n",
    "                mae = np.mean(np.abs(aligned_truth - aligned_pred))\n",
    "                rmse = np.sqrt(np.mean((aligned_truth - aligned_pred)**2))\n",
    "                mape = np.mean(np.abs((aligned_truth - aligned_pred) / aligned_truth)) * 100\n",
    "\n",
    "                target_metrics[h_label] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'mape': mape,\n",
    "                    'pred_mean': np.mean(aligned_pred),\n",
    "                    'truth_mean': np.mean(aligned_truth)\n",
    "                }\n",
    "\n",
    "                print(f\"  {h_label:>3} ahead: MAE={mae:6.1f} MW, RMSE={rmse:6.1f} MW, MAPE={mape:5.1f}%\")\n",
    "\n",
    "        metrics_summary[target] = target_metrics\n",
    "\n",
    "    # Show horizon degradation\n",
    "    print(f\"\\nüìâ PREDICTION QUALITY DEGRADATION:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for target in ['offshore', 'onshore']:\n",
    "        if target in metrics_summary:\n",
    "            mae_1h = metrics_summary[target]['1h']['mae']\n",
    "            mae_24h = metrics_summary[target]['24h']['mae']\n",
    "            degradation = ((mae_24h - mae_1h) / mae_1h) * 100\n",
    "\n",
    "            print(f\"{target.capitalize():>8}: {mae_1h:.1f}‚Üí{mae_24h:.1f} MW (+{degradation:.1f}% error increase)\")\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics_summary,\n",
    "        'continuous_predictions': continuous_preds\n",
    "    }\n",
    "\n",
    "def quick_evaluation(denorm_preds, df_combined, target_col=\"wind_power_offshore\"):\n",
    "    \"\"\"\n",
    "    Quick performance evaluation\n",
    "    \"\"\"\n",
    "    pred = denorm_preds[target_col]\n",
    "    truth = df_combined[target_col].values\n",
    "\n",
    "    # Calculate metrics for first 100 windows\n",
    "    maes = []\n",
    "    for i in range(min(100, pred.shape[0])):\n",
    "        if i + 24 <= len(truth):\n",
    "            gt_window = truth[i:i+24]\n",
    "            pred_window = pred[i]\n",
    "            maes.append(np.mean(np.abs(gt_window - pred_window)))\n",
    "\n",
    "    avg_mae = np.mean(maes)\n",
    "\n",
    "    print(f\"üéØ {target_col} Performance:\")\n",
    "    print(f\"  Average MAE: {avg_mae:.1f} MW\")\n",
    "    print(f\"  Prediction mean: {pred.mean():.1f} MW\")\n",
    "    print(f\"  Ground truth mean: {np.mean(truth):.1f} MW\")\n",
    "    print(f\"  Error as % of mean: {avg_mae/np.mean(truth)*100:.1f}%\")\n",
    "\n",
    "    return avg_mae\n",
    "\n",
    "# Usage:\n",
    "# denorm_preds = denormalize_ttm_predictions_simple(predictions_np, tsp)\n",
    "# results = plot_clean_comparison(denorm_preds, df_test, start_idx=0, length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "izjAyY1txhGF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1217,
     "status": "ok",
     "timestamp": 1750151982490,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "izjAyY1txhGF",
    "outputId": "4322faf5-eb92-40ff-f717-8fb3a41c0827"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_experiment_metadata(result, ttm_model, dset_test, predictions_np,\n",
    "                               batch_size=64, model_type=\"zero-shot\", model_path=None,\n",
    "                               dataset_name=\"german_wind_power\", spatial_coverage=\"germany_averaged\"):\n",
    "    \"\"\"\n",
    "    Extract experiment-specific metadata for evaluation pipeline tracking\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result : dict\n",
    "        Output from prepare_wind_power_dataset()\n",
    "    ttm_model : model\n",
    "        The TTM model instance\n",
    "    dset_test : dataset\n",
    "        Test dataset\n",
    "    predictions_np : numpy array\n",
    "        Raw model predictions\n",
    "    batch_size : int\n",
    "        Batch size used for inference\n",
    "    model_type : str\n",
    "        \"zero-shot\", \"fine-tuned\", etc.\n",
    "    model_path : str\n",
    "        Path to model weights\n",
    "    dataset_name : str\n",
    "        Name of dataset (configurable)\n",
    "    spatial_coverage : str\n",
    "        Spatial coverage description (configurable)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Experiment-specific metadata only\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîç EXTRACTING EXPERIMENT METADATA\")\n",
    "    print(\"=\"*45)\n",
    "\n",
    "    tsp = result['preprocessor']\n",
    "\n",
    "    # 1. Feature configuration (varies between experiments)\n",
    "    feature_config = {\n",
    "        'target_columns': tsp.target_columns.copy(),\n",
    "        'observable_columns': getattr(tsp, 'observable_columns', []).copy(),\n",
    "        'conditional_columns': getattr(tsp, 'conditional_columns', []).copy(),\n",
    "        'num_targets': len(tsp.target_columns),\n",
    "        'num_observables': len(getattr(tsp, 'observable_columns', [])),\n",
    "        'num_total_features': len(tsp.target_columns) + len(getattr(tsp, 'observable_columns', [])) + len(getattr(tsp, 'conditional_columns', [])),\n",
    "        'scaling_enabled': getattr(tsp, 'scaling', True),\n",
    "        'scaler_type': getattr(tsp, 'scaler_type', 'standard')\n",
    "    }\n",
    "\n",
    "    # 2. Model and training hyperparameters (varies between experiments)\n",
    "    model_config = {\n",
    "        'model_type': model_type,\n",
    "        'model_path': model_path,\n",
    "        'context_length': result['context_length'],\n",
    "        'prediction_length': result['prediction_length'],\n",
    "        'batch_size': batch_size,\n",
    "        'freq_prefix_tuning': getattr(ttm_model.config, 'freq_prefix_tuning', False),\n",
    "        'resolution_prefix_tuning': getattr(ttm_model.config, 'resolution_prefix_tuning', False)\n",
    "    }\n",
    "\n",
    "    # 3. Prediction metadata (varies between experiments)\n",
    "    prediction_info = {\n",
    "        'prediction_shape': list(predictions_np.shape),\n",
    "        'num_windows': predictions_np.shape[0],\n",
    "        'horizon_length': predictions_np.shape[1],\n",
    "        'num_channels': predictions_np.shape[2],\n",
    "        'test_windows': len(dset_test)\n",
    "    }\n",
    "\n",
    "    # 4. Experiment tracking info\n",
    "    experiment_info = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'experiment_id': generate_experiment_id(model_config, feature_config),\n",
    "        'run_name': generate_run_name(model_type, model_config, feature_config),\n",
    "        'dataset_name': dataset_name,\n",
    "        'spatial_coverage': spatial_coverage\n",
    "    }\n",
    "\n",
    "    # Combine all metadata (experiment-specific only)\n",
    "    metadata = {\n",
    "        'feature_config': feature_config,\n",
    "        'model_config': model_config,\n",
    "        'prediction_info': prediction_info,\n",
    "        'experiment_info': experiment_info\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print_metadata_summary(metadata)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def generate_experiment_id(model_config, feature_config):\n",
    "    \"\"\"Generate timestamp-based experiment ID\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    return timestamp\n",
    "\n",
    "def generate_run_name(model_type, model_config, feature_config):\n",
    "    \"\"\"Generate human-readable run name\"\"\"\n",
    "    return f\"{model_type}_cxt{model_config['context_length']}_pred{model_config['prediction_length']}_f{feature_config['num_total_features']}\"\n",
    "\n",
    "def print_metadata_summary(metadata):\n",
    "    \"\"\"Print a concise summary of the extracted metadata\"\"\"\n",
    "\n",
    "    print(f\"\\nüìã EXPERIMENT METADATA SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    exp = metadata['experiment_info']\n",
    "    features = metadata['feature_config']\n",
    "    model = metadata['model_config']\n",
    "    pred = metadata['prediction_info']\n",
    "\n",
    "    print(f\"üÜî Experiment ID: {exp['experiment_id']}\")\n",
    "    print(f\"üè∑Ô∏è  Run Name: {exp['run_name']}\")\n",
    "    print(f\"üìÖ Timestamp: {exp['timestamp'][:19]}\")\n",
    "    print(f\"üó∫Ô∏è  Coverage: {exp['spatial_coverage']}\")\n",
    "\n",
    "    print(f\"\\nüéØ Features:\")\n",
    "    print(f\"   Targets: {features['num_targets']} {features['target_columns']}\")\n",
    "    print(f\"   Observables: {features['num_observables']}\")\n",
    "    print(f\"   Total features: {features['num_total_features']}\")\n",
    "    print(f\"   Scaling: {features['scaler_type'] if features['scaling_enabled'] else 'none'}\")\n",
    "\n",
    "    print(f\"\\nü§ñ Model: {model['model_type']}\")\n",
    "    print(f\"   Context: {model['context_length']}h ({model['context_length']/24:.1f} days)\")\n",
    "    print(f\"   Prediction: {model['prediction_length']}h\")\n",
    "    print(f\"   Batch size: {model['batch_size']}\")\n",
    "\n",
    "    print(f\"\\nüîÆ Predictions: {pred['prediction_shape']}\")\n",
    "    print(f\"   ({pred['num_windows']:,} windows, {pred['horizon_length']}h horizon, {pred['num_channels']} channels)\")\n",
    "\n",
    "def compute_multihorizon_metrics(predictions_denormalized, ground_truth, target_columns, horizons=None):\n",
    "    \"\"\"\n",
    "    Compute comprehensive metrics for all forecast horizons\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_denormalized : dict\n",
    "        Output from denormalize_ttm_predictions_simple()\n",
    "    ground_truth : dict\n",
    "        Ground truth arrays for each target\n",
    "    target_columns : list\n",
    "        List of target column names\n",
    "    horizons : list, optional\n",
    "        Forecast horizons to evaluate (in hours, 1-indexed). If None, uses all 24 hours.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Metrics organized by target and horizon\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüìä COMPUTING MULTI-HORIZON METRICS\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Default to all 24 forecast horizons\n",
    "    if horizons is None:\n",
    "        horizons = list(range(1, 25))  # 1h, 2h, 3h, ..., 24h ahead\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        if target not in predictions_denormalized or target not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        pred_array = predictions_denormalized[target]  # (n_windows, 24)\n",
    "        truth_array = ground_truth[target]\n",
    "\n",
    "        target_metrics = {}\n",
    "\n",
    "        print(f\"\\n  {target}:\")\n",
    "\n",
    "        for h in horizons:\n",
    "            h_idx = h - 1  # Convert to 0-indexed\n",
    "\n",
    "            if h_idx >= pred_array.shape[1]:\n",
    "                continue\n",
    "\n",
    "            # Extract h-hour ahead predictions\n",
    "            pred_h = pred_array[:, h_idx]\n",
    "\n",
    "            # Align with ground truth (account for forecast horizon offset)\n",
    "            max_len = min(len(truth_array) - h, len(pred_h))\n",
    "\n",
    "            if max_len <= 0:\n",
    "                continue\n",
    "\n",
    "            aligned_truth = truth_array[h:h + max_len]\n",
    "            aligned_pred = pred_h[:max_len]\n",
    "\n",
    "            # Calculate metrics (only MAE and RMSE)\n",
    "            mae = np.mean(np.abs(aligned_truth - aligned_pred))\n",
    "            rmse = np.sqrt(np.mean((aligned_truth - aligned_pred)**2))\n",
    "\n",
    "            target_metrics[f'{h}h'] = {\n",
    "                'mae': float(mae),\n",
    "                'rmse': float(rmse)\n",
    "            }\n",
    "\n",
    "            # Print every 4th hour to avoid clutter\n",
    "            if h % 4 == 1 or h == 24:\n",
    "                print(f\"    {h:>2}h ahead: MAE={mae:6.1f} MW, RMSE={rmse:6.1f} MW\")\n",
    "\n",
    "        # Overall metrics (average across all horizons)\n",
    "        if target_metrics:\n",
    "            overall_mae = np.mean([m['mae'] for m in target_metrics.values()])\n",
    "            overall_rmse = np.mean([m['rmse'] for m in target_metrics.values()])\n",
    "\n",
    "            target_metrics['overall'] = {\n",
    "                'mae': float(overall_mae),\n",
    "                'rmse': float(overall_rmse)\n",
    "            }\n",
    "\n",
    "            print(f\"    Overall: MAE={overall_mae:6.1f} MW, RMSE={overall_rmse:6.1f} MW\")\n",
    "\n",
    "        metrics[target] = target_metrics\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def prepare_evaluation_data(metadata, predictions_np, df_combined, tsp, split_config,\n",
    "                          out_dir=\"results/\", save_results=True):\n",
    "    \"\"\"\n",
    "    Prepare data, compute metrics, and save evaluation results\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    metadata : dict\n",
    "        Experiment metadata\n",
    "    predictions_np : numpy array\n",
    "        Raw model predictions\n",
    "    df_combined : pandas.DataFrame\n",
    "        Full combined dataset\n",
    "    tsp : TimeSeriesPreprocessor\n",
    "        Fitted preprocessor\n",
    "    split_config : dict\n",
    "        Train/val/test split configuration\n",
    "    out_dir : str\n",
    "        Base output directory\n",
    "    save_results : bool\n",
    "        Whether to save results to disk\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Complete evaluation results\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüîß PREPARING EVALUATION DATA\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Extract test data\n",
    "    test_start, test_end = split_config['test']\n",
    "    df_test = df_combined.iloc[test_start:test_end].copy()\n",
    "\n",
    "    # Prepare ground truth arrays\n",
    "    ground_truth = {}\n",
    "    for target in metadata['feature_config']['target_columns']:\n",
    "        if target in df_test.columns:\n",
    "            ground_truth[target] = df_test[target].values\n",
    "\n",
    "    # Denormalize predictions (function should be defined in jupyter cell above)\n",
    "    denormalized_predictions = denormalize_ttm_predictions_simple(predictions_np, tsp)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_multihorizon_metrics(\n",
    "        denormalized_predictions,\n",
    "        ground_truth,\n",
    "        metadata['feature_config']['target_columns']\n",
    "    )\n",
    "\n",
    "    evaluation_results = {\n",
    "        'metadata': metadata,\n",
    "        'metrics': metrics,\n",
    "        'predictions_raw': predictions_np,\n",
    "        'predictions_denormalized': denormalized_predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'test_timestamps': df_test['timestamp'].values\n",
    "    }\n",
    "\n",
    "    if save_results:\n",
    "        save_evaluation_results(evaluation_results, out_dir)\n",
    "\n",
    "    print(f\"\\n‚úÖ Evaluation complete:\")\n",
    "    print(f\"   Targets evaluated: {list(metrics.keys())}\")\n",
    "    print(f\"   Horizons per target: {len([k for k in list(metrics.values())[0].keys() if k != 'overall'])}\")\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "def save_evaluation_results(evaluation_results, out_dir=\"results/\"):\n",
    "    \"\"\"\n",
    "    Save evaluation results to structured directory\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    evaluation_results : dict\n",
    "        Complete evaluation results from prepare_evaluation_data()\n",
    "    out_dir : str\n",
    "        Base output directory\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüíæ SAVING EVALUATION RESULTS\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    metadata = evaluation_results['metadata']\n",
    "    metrics = evaluation_results['metrics']\n",
    "\n",
    "    # Create base output directory\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Create experiment-specific directory\n",
    "    exp_id = metadata['experiment_info']['experiment_id']\n",
    "    run_name = metadata['experiment_info']['run_name']\n",
    "    exp_dir = out_path / f\"{exp_id}_{run_name}\"\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"üìÅ Experiment directory: {exp_dir}\")\n",
    "\n",
    "    # 1. Save metadata as structured CSV\n",
    "    metadata_df = flatten_metadata_to_dataframe(metadata)\n",
    "    metadata_path = exp_dir / \"metadata.csv\"\n",
    "    metadata_df.to_csv(metadata_path, index=False)\n",
    "    print(f\"   ‚úÖ Metadata saved: {metadata_path.name}\")\n",
    "\n",
    "    # 2. Save metrics as structured CSV\n",
    "    metrics_df = flatten_metrics_to_dataframe(metrics)\n",
    "    metrics_path = exp_dir / \"metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"   ‚úÖ Metrics saved: {metrics_path.name}\")\n",
    "\n",
    "    # 3. Save predictions as npz\n",
    "    predictions_path = exp_dir / \"predictions.npz\"\n",
    "    np.savez_compressed(\n",
    "        predictions_path,\n",
    "        raw_predictions=evaluation_results['predictions_raw'],\n",
    "        **evaluation_results['predictions_denormalized'],  # Save each target separately\n",
    "        timestamps=evaluation_results['test_timestamps']\n",
    "    )\n",
    "    print(f\"   ‚úÖ Predictions saved: {predictions_path.name}\")\n",
    "\n",
    "    # 4. Save ground truth once (check if exists)\n",
    "    ground_truth_path = out_path / \"ground_truth.npz\"\n",
    "    if not ground_truth_path.exists():\n",
    "        np.savez_compressed(\n",
    "            ground_truth_path,\n",
    "            **evaluation_results['ground_truth'],\n",
    "            timestamps=evaluation_results['test_timestamps']\n",
    "        )\n",
    "        print(f\"   ‚úÖ Ground truth saved: {ground_truth_path.name}\")\n",
    "    else:\n",
    "        print(f\"   ‚è≠Ô∏è  Ground truth exists: {ground_truth_path.name}\")\n",
    "\n",
    "    print(f\"\\nüìÅ All results saved in: {exp_dir}\")\n",
    "    return exp_dir\n",
    "\n",
    "def flatten_metadata_to_dataframe(metadata):\n",
    "    \"\"\"\n",
    "    Convert nested metadata dict to flat DataFrame for easy CSV reading\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Flatten each section\n",
    "    for section_name, section_data in metadata.items():\n",
    "        if isinstance(section_data, dict):\n",
    "            for key, value in section_data.items():\n",
    "                if isinstance(value, (list, tuple)):\n",
    "                    # Convert lists to string representation\n",
    "                    value = str(value)\n",
    "\n",
    "                rows.append({\n",
    "                    'section': section_name,\n",
    "                    'parameter': key,\n",
    "                    'value': value\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def flatten_metrics_to_dataframe(metrics):\n",
    "    \"\"\"\n",
    "    Convert nested metrics dict to flat DataFrame for easy analysis\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for target, target_metrics in metrics.items():\n",
    "        for horizon, horizon_metrics in target_metrics.items():\n",
    "            if isinstance(horizon_metrics, dict):\n",
    "                for metric_name, metric_value in horizon_metrics.items():\n",
    "                    rows.append({\n",
    "                        'target': target,\n",
    "                        'horizon': horizon,\n",
    "                        'metric': metric_name,\n",
    "                        'value': metric_value\n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Pivot for easier reading: columns = metrics, rows = target_horizon combinations\n",
    "    if not df.empty:\n",
    "        df_pivot = df.pivot_table(\n",
    "            index=['target', 'horizon'],\n",
    "            columns='metric',\n",
    "            values='value',\n",
    "            fill_value=None\n",
    "        ).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        df_pivot.columns.name = None\n",
    "\n",
    "        return df_pivot\n",
    "\n",
    "    return df\n",
    "\n",
    "metadata = extract_experiment_metadata(\n",
    "    result=result,\n",
    "    ttm_model=zeroshot_model,\n",
    "    dset_test=dset_test,\n",
    "    predictions_np=predictions_np,\n",
    "    batch_size=batch_size,\n",
    "    model_type=\"zero-shot\",\n",
    "    model_path=TTM_MODEL_PATH,\n",
    "    dataset_name=\"german_wind_power\",\n",
    "    spatial_coverage=\"germany_averaged\"  # Pass as parameter\n",
    ")\n",
    "\n",
    "eval_results = prepare_evaluation_data(\n",
    "    metadata, predictions_np, df_combined, tsp, split_config,\n",
    "    out_dir=\"results/\", save_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cY4Q75bM89eh",
   "metadata": {
    "id": "cY4Q75bM89eh"
   },
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q0W_QKj79W4o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "executionInfo": {
     "elapsed": 249840,
     "status": "ok",
     "timestamp": 1750151944804,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "Q0W_QKj79W4o",
    "outputId": "edb5dfb2-4ef1-4a05-81b0-79578ececcd1"
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model with improved configuration\n",
    "print(\"üöÄ FINE-TUNING TTM MODEL\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Training configuration\n",
    "learning_rate = 0.001  # Higher learning rate as per TTM recommendations\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "print(f\"Using learning rate = {learning_rate}\")\n",
    "\n",
    "# Create temporary directory for training artifacts\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"ttm_finetune_\")\n",
    "print(f\"üìÅ Training in: {temp_dir}\")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "model_type = \"finetuned\"\n",
    "model_path = TTM_MODEL_PATH\n",
    "\n",
    "model_config = {\n",
    "    'model_type': model_type,\n",
    "    'model_path': model_path,\n",
    "    'context_length': result['context_length'],\n",
    "    'prediction_length': result['prediction_length'],\n",
    "    # 'batch_size': batch_size,\n",
    "    # 'freq_prefix_tuning': getattr(ttm_model.config, 'freq_prefix_tuning', False),\n",
    "    # 'resolution_prefix_tuning': getattr(ttm_model.config, 'resolution_prefix_tuning', False)\n",
    "}\n",
    "feature_config = {\n",
    "      'target_columns': tsp.target_columns.copy(),\n",
    "      # 'observable_columns': getattr(tsp, 'observable_columns', []).copy(),\n",
    "      # 'conditional_columns': getattr(tsp, 'conditional_columns', []).copy(),\n",
    "      'num_targets': len(tsp.target_columns),\n",
    "      'num_observables': len(getattr(tsp, 'observable_columns', [])),\n",
    "      'num_total_features': len(tsp.target_columns) + len(getattr(tsp, 'observable_columns', [])) + len(getattr(tsp, 'conditional_columns', [])),\n",
    "      # 'scaling_enabled': getattr(tsp, 'scaling', True),\n",
    "      # 'scaler_type': getattr(tsp, 'scaler_type', 'standard')\n",
    "  }\n",
    "\n",
    "\n",
    "# Training arguments (adapted from TTM example)\n",
    "training_args = TrainingArguments(\n",
    "    run_name=generate_run_name(model_type, model_config, feature_config),\n",
    "    output_dir=temp_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_epochs,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",  # <-- Use this instead of evaluation_strategy\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    dataloader_num_workers=4,  # Reduced for stability\n",
    "    report_to=None,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=os.path.join(temp_dir, \"logs\"),\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Create callbacks\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,  # Reasonable patience for our epochs\n",
    "    early_stopping_threshold=0.0,\n",
    ")\n",
    "\n",
    "# Check if TrackingCallback is available\n",
    "try:\n",
    "    tracking_callback = TrackingCallback()\n",
    "    callbacks = [early_stopping_callback, tracking_callback]\n",
    "except:\n",
    "    callbacks = [early_stopping_callback]\n",
    "    print(\"TrackingCallback not available, using only early stopping\")\n",
    "\n",
    "# Custom optimizer and scheduler (as per TTM recommendations)\n",
    "optimizer = AdamW(zeroshot_model.parameters(), lr=learning_rate)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=math.ceil(len(dset_train) / batch_size),\n",
    ")\n",
    "\n",
    "finetuned_model = get_model(\n",
    "    TTM_MODEL_PATH,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_length,\n",
    "    freq_prefix_tuning=False,\n",
    "    freq=None,\n",
    "    prefer_l1_loss=False,\n",
    "    prefer_longer_context=True,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,  # This will be fine-tuned in place\n",
    "    args=training_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_valid,\n",
    "    callbacks=callbacks,\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(f\"üèÉ Training for max {num_epochs} epochs...\")\n",
    "print(f\"   Train samples: {len(dset_train):,}\")\n",
    "print(f\"   Val samples: {len(dset_valid):,}\")\n",
    "print(f\"   Steps per epoch: {math.ceil(len(dset_train) / batch_size)}\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"‚úÖ Training completed!\")\n",
    "print(f\"   Final loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UAiEJ_twK8UV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 10903,
     "status": "ok",
     "timestamp": 1750151960182,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "UAiEJ_twK8UV",
    "outputId": "9bb89005-fc7c-41e9-c8f6-67627a6ab950"
   },
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print(f\"üîÆ Getting test predictions...\")\n",
    "predictions_dict = trainer.predict(dset_test)\n",
    "predictions_np = predictions_dict.predictions[0]\n",
    "print(f\"   Shape: {predictions_np.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Model fine-tuned! Ready for evaluation.\")\n",
    "print(f\"Use: predictions_np for the fine-tuned predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78lvRdmdLRyR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1438,
     "status": "ok",
     "timestamp": 1750152001058,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "78lvRdmdLRyR",
    "outputId": "06fcdb87-82a7-4f5a-a935-c0af8d8edd61"
   },
   "outputs": [],
   "source": [
    "metadata = extract_experiment_metadata(\n",
    "    result=result,\n",
    "    ttm_model=zeroshot_model, # finetuned_model\n",
    "    dset_test=dset_test,\n",
    "    predictions_np=predictions_np,\n",
    "    batch_size=batch_size,\n",
    "    model_type=model_type,\n",
    "    model_path=TTM_MODEL_PATH,\n",
    "    dataset_name=\"german_wind_power\",\n",
    "    spatial_coverage=\"germany_averaged\"\n",
    ")\n",
    "\n",
    "eval_results = prepare_evaluation_data(\n",
    "    metadata, predictions_np, df_combined, tsp, split_config,\n",
    "    out_dir=\"results/\", save_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fqWUV2w0D4Pn",
   "metadata": {
    "id": "fqWUV2w0D4Pn"
   },
   "outputs": [],
   "source": [
    "# Let‚Äôs say your application needs to forecast 24 hours in the future.\n",
    "# You can still use the 512-96 TTM model and set `prediction_filter_length=24` argument during model loading.\n",
    "# Try it on etth1, and note the evaluation error (on all channels)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WnbORdNcEBj2",
   "metadata": {
    "id": "WnbORdNcEBj2"
   },
   "outputs": [],
   "source": [
    "# In your notebook, add `prediction_channel_indices=[0,2]`\n",
    "# during model loading to forecast only 0th and 2nd channels.\n",
    "# In this case, execute the following code and note the output shape.\n",
    "# from tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\n",
    "# zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\"ibm/TTM\", revision=TTM_MODEL_REVISION, prediction_channel_indices=[0,2])\n",
    "# output = zeroshot_model.forward(test_dataset[0]['past_values'].unsqueeze(0), return_loss=False)\n",
    "# output.prediction_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vm8Rpni09y7k",
   "metadata": {
    "id": "Vm8Rpni09y7k"
   },
   "source": [
    "### Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R8Apo_l191ZZ",
   "metadata": {
    "id": "R8Apo_l191ZZ"
   },
   "outputs": [],
   "source": [
    "!pip install -q streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JST7KNCR-XNm",
   "metadata": {
    "id": "JST7KNCR-XNm"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py &>/content/drive/MyDrive/ttm/logs.txt &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NGRP8Sj6-C2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282416,
     "status": "ok",
     "timestamp": 1750148739190,
     "user": {
      "displayName": "Lukas Mayer",
      "userId": "09672943141921097160"
     },
     "user_tz": -120
    },
    "id": "NGRP8Sj6-C2a",
    "outputId": "2d8cfe8d-2f31-45de-e035-7cbd6f6844c2"
   },
   "outputs": [],
   "source": [
    "# !npm install localtunnel\n",
    "!npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCbi8FGa_lpx",
   "metadata": {
    "id": "NCbi8FGa_lpx"
   },
   "outputs": [],
   "source": [
    "# !pip install pyngrok\n",
    "# !ngrok authtoken <YOUR_AUTHTOKEN>\n",
    "# !ngrok http 8501\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2fJJa400Z_D",
   "metadata": {
    "id": "p2fJJa400Z_D"
   },
   "source": [
    "### tsfm Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7115a87-a9c7-41c1-b92b-96dee59d948a",
   "metadata": {
    "id": "b7115a87-a9c7-41c1-b92b-96dee59d948a"
   },
   "outputs": [],
   "source": [
    "# get backbone embeddings (if needed for further analysis)\n",
    "# backbone_embedding = predictions_dict.predictions[1]\n",
    "# print(backbone_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf67294-2996-4b0e-8cdd-9a0a67acb26f",
   "metadata": {
    "id": "bcf67294-2996-4b0e-8cdd-9a0a67acb26f",
    "outputId": "9fa8a683-1fc8-47e2-f32f-150135bb229d"
   },
   "outputs": [],
   "source": [
    "# TODO: Show complete context - should be 512 + 24?\n",
    "# plot\n",
    "plot_predictions(\n",
    "    model=zeroshot_trainer.model,\n",
    "    dset=dset_test,\n",
    "    plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "    plot_prefix=\"test_zeroshot\",\n",
    "    # indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "    channel=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923a8ec-c0a0-405d-8376-1c1ec1eaaa5d",
   "metadata": {
    "id": "5923a8ec-c0a0-405d-8376-1c1ec1eaaa5d",
    "outputId": "0cd37f1f-e141-4766-971c-6c7e5128bb2e"
   },
   "outputs": [],
   "source": [
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a172b9-3141-4dfa-a616-e91767ffb04f",
   "metadata": {
    "id": "e5a172b9-3141-4dfa-a616-e91767ffb04f"
   },
   "outputs": [],
   "source": [
    "TARGET_DATASET = \"etth1\"\n",
    "example_dataset_path = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\"\n",
    "\n",
    "timestamp_column = \"date\"\n",
    "id_columns = []  # mention the ids that uniquely identify a time-series.\n",
    "\n",
    "target_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "split_config = {\n",
    "    \"train\": [0, 8640],\n",
    "    \"valid\": [8640, 11520],\n",
    "    \"test\": [\n",
    "        11520,\n",
    "        14400,\n",
    "    ],\n",
    "}\n",
    "# Understanding the split config -- slides\n",
    "\n",
    "example_data = pd.read_csv(\n",
    "    example_dataset_path,\n",
    "    parse_dates=[timestamp_column],\n",
    ")\n",
    "\n",
    "column_specifiers = {\n",
    "    \"timestamp_column\": timestamp_column,\n",
    "    \"id_columns\": id_columns,\n",
    "    \"target_columns\": target_columns,\n",
    "    \"control_columns\": [],\n",
    "}\n",
    "\n",
    "# def zeroshot_eval(dataset_name, batch_size, context_length=512, forecast_length=96):\n",
    "    # Get data\n",
    "# zeroshot_eval(\n",
    "#     dataset_name=TARGET_DATASET, context_length=CONTEXT_LENGTH, forecast_length=PREDICTION_LENGTH, batch_size=64\n",
    "# )\n",
    "dataset_name=TARGET_DATASET\n",
    "context_length=CONTEXT_LENGTH\n",
    "forecast_length=PREDICTION_LENGTH\n",
    "batch_size=64\n",
    "\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    **column_specifiers,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_length,\n",
    "    scaling=True,\n",
    "    encode_categorical=False,\n",
    "    scaler_type=\"standard\",\n",
    ")\n",
    "\n",
    "# Load model\n",
    "zeroshot_model = get_model(\n",
    "    TTM_MODEL_PATH,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_length,\n",
    "    freq_prefix_tuning=False,\n",
    "    freq=None,\n",
    "    prefer_l1_loss=False,\n",
    "    prefer_longer_context=True,\n",
    ")\n",
    "\n",
    "dset_train, dset_valid, dset_test = get_datasets(\n",
    "    tsp, data, split_config, use_frequency_token=zeroshot_model.config.resolution_prefix_tuning\n",
    ")\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "# zeroshot_trainer\n",
    "zeroshot_trainer = Trainer(\n",
    "    model=zeroshot_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=temp_dir,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "# evaluate = zero-shot performance\n",
    "print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "print(zeroshot_output)\n",
    "\n",
    "# get predictions\n",
    "\n",
    "predictions_dict = zeroshot_trainer.predict(dset_test)\n",
    "\n",
    "predictions_np = predictions_dict.predictions[0]\n",
    "\n",
    "print(predictions_np.shape)\n",
    "\n",
    "# get backbone embeddings (if needed for further analysis)\n",
    "\n",
    "backbone_embedding = predictions_dict.predictions[1]\n",
    "\n",
    "print(backbone_embedding.shape)\n",
    "\n",
    "# plot\n",
    "plot_predictions(\n",
    "    model=zeroshot_trainer.model,\n",
    "    dset=dset_test,\n",
    "    plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "    plot_prefix=\"test_zeroshot\",\n",
    "    indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "    channel=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228b369",
   "metadata": {
    "id": "f228b369"
   },
   "outputs": [],
   "source": [
    "# Input:      (8737, 512, 16)     # Raw time series\n",
    "# Patchify:   (8737, 16, 8, 64)   # 64-timestep patches\n",
    "# Embed:      (8737, 16, 8, 192)  # Project to 192 dims\n",
    "# Block 0:    (8737, 16, 36, 48)  # Expand patches, reduce dims\n",
    "# Block 1:    (8737, 16, 18, 96)  # Reduce patches, expand dims\n",
    "# Block 2:    (8737, 16, 9, 192)  # üéØ FINAL: 9 patches √ó 192 dims\n",
    "# Decoder:    (8737, 16, 9, 128)  # Reduce to 128 dims\n",
    "# Head:       (8737, 48, 16)      # Flatten: 9√ó128=1152 ‚Üí predict 48 hours\n",
    "# Filter:     (8737, 24, 16)      # Keep first 24 hours"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "54e2e45d-d4ef-4b75-872e-ae92c8d159b7",
    "54823b31-b519-4701-9759-e18540dc5b0b",
    "Vm8Rpni09y7k",
    "p2fJJa400Z_D"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f3c3a65081940fca2853f6cebbe84b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2188739422404aa9865e58cc66f7fc8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23824dcdd2d4432c86efd8c6d6eaaa37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cb90eb7730a41afac3f51b906f585be",
      "max": 3219040,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3bb25e03a9c14d9d8a145ac6c84ee601",
      "value": 3219040
     }
    },
    "2cdf6461a1c64f0d9a2d6dc55e62b5a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e342bc24924404c954b57eceed2c08e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e6498723d964ead82ef0c1971265831": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bb25e03a9c14d9d8a145ac6c84ee601": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d6f6c3b7af2495c85cb5758b3ef0e58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f78e35d4d1254c8fb2172214bfdb4b1f",
       "IPY_MODEL_23824dcdd2d4432c86efd8c6d6eaaa37",
       "IPY_MODEL_6a4db443cce94842b00d3ab6de3193c5"
      ],
      "layout": "IPY_MODEL_2e342bc24924404c954b57eceed2c08e"
     }
    },
    "510b0d0e9c5a4df1888f6379b875ad49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a89c34371674fc0b1ce5068ebf526a4",
       "IPY_MODEL_f82a90d48fb94dd3bec2bd17c31ed2bc",
       "IPY_MODEL_e9ecdf4cb3ca4f29ae91248da74d62f2"
      ],
      "layout": "IPY_MODEL_2cdf6461a1c64f0d9a2d6dc55e62b5a0"
     }
    },
    "5a89c34371674fc0b1ce5068ebf526a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca49952ab2db4e3da15db67c03146934",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c2d6cdfdb6e242a5a4fbcc92c445d509",
      "value": "config.json:‚Äá100%"
     }
    },
    "5cb90eb7730a41afac3f51b906f585be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67216125c2d644b091cdbcb2133f0112": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a4db443cce94842b00d3ab6de3193c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0ff0b650c764d3f918545504274baca",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cf5b43e033214235ac4e54d293b2f68b",
      "value": "‚Äá3.22M/3.22M‚Äá[00:01&lt;00:00,‚Äá1.66MB/s]"
     }
    },
    "750fdcda161445feb5cdf259bf5c230d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8135ff20b9374d7a99070fad1691cb35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0ff0b650c764d3f918545504274baca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2d6cdfdb6e242a5a4fbcc92c445d509": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca49952ab2db4e3da15db67c03146934": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf5b43e033214235ac4e54d293b2f68b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9ecdf4cb3ca4f29ae91248da74d62f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e6498723d964ead82ef0c1971265831",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8135ff20b9374d7a99070fad1691cb35",
      "value": "‚Äá1.65k/1.65k‚Äá[00:00&lt;00:00,‚Äá90.3kB/s]"
     }
    },
    "f78e35d4d1254c8fb2172214bfdb4b1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_750fdcda161445feb5cdf259bf5c230d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0f3c3a65081940fca2853f6cebbe84b8",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "f82a90d48fb94dd3bec2bd17c31ed2bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2188739422404aa9865e58cc66f7fc8e",
      "max": 1650,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_67216125c2d644b091cdbcb2133f0112",
      "value": 1650
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
